{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgnaVVW442lT"
      },
      "source": [
        "#### Primary resources\n",
        "- LangChain: https://www.langchain.com/\n",
        "- LangChain chat memory: https://python.langchain.com/docs/how_to/chatbots_memory/\n",
        "\n",
        "\n",
        "#### Additional resources\n",
        "- How to use Google colab: https://colab.research.google.com/drive/16pBJQePbqkz3QFV54L4NIkOn1kwpuRrj"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enter your Google API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nV3GEBwH4urc",
        "outputId": "4dff0ebd-4fbc-4a28-8697-e63e6d9c13c5"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Qi97DW47NG",
        "outputId": "bd220aca-2383-4062-c0e1-bb07ee0e5bef"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet langchain langgraph langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LLM config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK5k7mqG5FqC"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "model = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash-001\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpwv38l46aEs"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, RemoveMessage, SystemMessage\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import START, MessagesState, StateGraph\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "workflow = StateGraph(state_schema=MessagesState)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpS3978y5Kf9"
      },
      "outputs": [],
      "source": [
        "def call_model(state: MessagesState):\n",
        "    system_prompt = (\n",
        "        \"You are a helpful assistant. \"\n",
        "        \"Answer all questions to the best of your ability. \"\n",
        "        \"The provided chat history includes a summary of the earlier conversation.\"\n",
        "    )\n",
        "\n",
        "    system_message = SystemMessage(content=system_prompt)\n",
        "    message_history = state[\"messages\"][:-1]\n",
        "\n",
        "    if len(message_history) >= 4:\n",
        "        last_human_message = state[\"messages\"][-1]\n",
        "        \n",
        "        # Invoke the model to generate conversation summary\n",
        "        summary_prompt = (\n",
        "            \"Distill the above chat messages into a single summary message. \"\n",
        "            \"Include as many specific details as you can.\"\n",
        "        )\n",
        "        summary_message = model.invoke(\n",
        "            message_history + [HumanMessage(content=summary_prompt)]\n",
        "        )\n",
        "\n",
        "        # Delete messages that we no longer want to show up\n",
        "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n",
        "        \n",
        "        # Re-add user message\n",
        "        human_message = HumanMessage(content=last_human_message.content)\n",
        "        \n",
        "        # Call the model with summary & response\n",
        "        response = model.invoke([system_message, summary_message, human_message])\n",
        "        message_updates = [summary_message, human_message, response] + delete_messages\n",
        "    else:\n",
        "        message_updates = model.invoke([system_message] + state[\"messages\"])\n",
        "\n",
        "    return {\"messages\": message_updates}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keh4LjUm5850"
      },
      "outputs": [],
      "source": [
        "workflow.add_node(\"model\", call_model)\n",
        "workflow.add_edge(START, \"model\")\n",
        "\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example for chat history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7lf2BnB5oBC"
      },
      "outputs": [],
      "source": [
        "chat_history_example = [\n",
        "    HumanMessage(content=\"Hey there! I'm Ricards.\"),\n",
        "    AIMessage(content=\"Hello!\"),\n",
        "    HumanMessage(content=\"How are you today?\"),\n",
        "    AIMessage(content=\"Fine thanks!\"),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ask question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5tdp-7W6DjV"
      },
      "outputs": [],
      "source": [
        "def query_llm(question):\n",
        "    ai_msg = app.invoke(\n",
        "        {\n",
        "            \"messages\": chat_history_example\n",
        "            + [HumanMessage(question)]\n",
        "        },\n",
        "        config={\"configurable\": {\"thread_id\": \"4\"}},\n",
        "    )\n",
        "    print(ai_msg[\"messages\"][-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqFjKNXD6ljw",
        "outputId": "968575a4-4d82-416c-c720-31f3570bc0c0"
      },
      "outputs": [],
      "source": [
        "question = input(\"Please enter your question: \")\n",
        "print(f\"Question: {question}\")\n",
        "\n",
        "query_llm(question)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
